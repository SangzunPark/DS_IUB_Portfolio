{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ee1413",
   "metadata": {},
   "source": [
    "# LLM_NoCustomizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1f11562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\piano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "This document focuses on the importance of organization and time management in today's fast-paced world. It recommends using technology such as calendars, task managers, and reminders to help prioritize tasks, set deadlines, and stay committed. The document also provides formatting guidelines for the preparation section, which outlines the steps necessary for preparing a task or project. Additionally, it includes a section on \"Equivalent/Alternative Plans\" which explores different approaches and solutions, providing flexibility and options for achieving the desired outcome.\n",
      "\n",
      "When it comes to summarizing the document, not all information needs to be addressed at length, especially if it is not relevant to the regional conditions or needed to define or substantiate goals, measurable objectives, or specific actions. Key trends should be identified and made briefly and clearly for a summary section. Supporting data can be moved to an appendix.\n",
      "\n",
      "Demographic and socioeconomic data, including the human capital assets of the area and labor force characteristics such as the educational attainment of the working age population, are relevant pieces of information that may be included in the document. Environmental, geographic, climatic, and cultural (including historic preservation) and natural resource profiles, such as mining resources, timber, fisheries, aquaculture, eco-tourism, etc., are also relevant. An environmental baseline for the area should be developed that identifies any environmental elements that may affect and/or constrain the regional economy.\n",
      "\n",
      "In summary, this document emphasizes the importance of thorough planning, considering different possibilities, and utilizing technology to ensure success in managing tasks and projects. It also provides guidelines for formatting the preparation section and includes relevant information such as demographic and socioeconomic data, and environmental and cultural profiles.\n",
      "\n",
      "Hallucination Rate: 8.33%\n",
      "\n",
      "Unsupported Sentences:\n",
      "1. It also provides guidelines for formatting the preparation section and includes relevant information such as demographic and socioeconomic data, and environmental and cultural profiles.\n",
      "   Explanation: no. the sentence does not align with the context. the provided sentence seems to focus on formatting and general inclusion of demographic and socioeconomic data, environmental, and cultural profiles rather than highlighting the importance of identifying key trends and presenting them succinctly in a summary section while moving supporting data to an appendix.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.docstore.document import Document\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set paths for multiple PDF files\n",
    "pdf_paths = [\"Contents/eda_ceds_guidelines_2023.pdf\"]\n",
    "\n",
    "# Load PDF files\n",
    "documents = []\n",
    "for path in pdf_paths:\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "# --- GPT-3.5 Based Chunking ---\n",
    "def gpt_chunking(documents, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Use GPT-3.5 to semantically chunk the text from documents.\n",
    "    Each chunk is generated based on GPT's understanding of context.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    for doc in documents:\n",
    "        sentences = sent_tokenize(doc.page_content)\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # Add sentence to the current chunk if it doesn't exceed the chunk size\n",
    "            if current_length + len(sentence) <= chunk_size:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length += len(sentence)\n",
    "            else:\n",
    "                # Process the current chunk with GPT to refine it\n",
    "                refined_chunk = process_with_gpt(\" \".join(current_chunk))\n",
    "                chunks.append(Document(page_content=refined_chunk))\n",
    "                \n",
    "                # Start a new chunk\n",
    "                current_chunk = [sentence]\n",
    "                current_length = len(sentence)\n",
    "\n",
    "        # Process the final chunk\n",
    "        if current_chunk:\n",
    "            refined_chunk = process_with_gpt(\" \".join(current_chunk))\n",
    "            chunks.append(Document(page_content=refined_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def process_with_gpt(text):\n",
    "    \"\"\"\n",
    "    Use GPT-3.5 to refine and validate a chunk of text.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Given the following text:\\n\\n{text}\\n\\n\"\n",
    "        \"Split this text into a coherent chunk that makes sense contextually. \"\n",
    "        \"Make sure the chunk captures the main idea and is self-contained.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=500\n",
    "        )\n",
    "        chunk = response['choices'][0]['message']['content'].strip()\n",
    "        return chunk\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing with GPT: {e}\")\n",
    "        return text  # Fallback to original text if GPT processing fails\n",
    "\n",
    "# Perform GPT-based chunking\n",
    "texts = gpt_chunking(documents)\n",
    "\n",
    "# Initialize embedding model for LangChain\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vectorstore = Chroma.from_documents(texts, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Initialize the LLM model\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "llm = ChatMistralAI(api_key=api_key)\n",
    "\n",
    "# Create a RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "# Define the query for summarization\n",
    "query = \"Summarize this document in about 10000 characters\"\n",
    "summary = qa_chain.run(query)\n",
    "\n",
    "print(\"Generated Summary:\")\n",
    "print(summary)\n",
    "\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# --- OpenAI GPT Based Hallucination ---\n",
    "def detect_hallucinations_with_gpt(summary, retriever, max_docs=3):\n",
    "    \"\"\"\n",
    "    Use OpenAI GPT to evaluate if each sentence in the summary aligns with the source documents.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(summary)\n",
    "    unsupported_sentences = []\n",
    "    supported_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Retrieve relevant documents\n",
    "        relevant_docs = retriever.get_relevant_documents(sentence)[:max_docs]\n",
    "        context = \" \".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "        # Query OpenAI GPT to check alignment\n",
    "        prompt = (\n",
    "            f\"Given the following context:\\n\\n{context}\\n\\n\"\n",
    "            f\"Does this sentence align with the context? Answer 'Yes' or 'No', and briefly explain:\\n\\n\"\n",
    "            f\"Sentence: \\\"{sentence}\\\"\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # Updated OpenAI API usage\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            answer = response['choices'][0]['message']['content'].strip().lower()\n",
    "            \n",
    "            if \"yes\" in answer:\n",
    "                supported_sentences.append((sentence, answer))\n",
    "            else:\n",
    "                unsupported_sentences.append((sentence, answer))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error querying OpenAI GPT: {e}\")\n",
    "            unsupported_sentences.append((sentence, \"Error: Unable to evaluate\"))\n",
    "\n",
    "    hallucination_rate = len(unsupported_sentences) / len(sentences) * 100\n",
    "    return hallucination_rate, unsupported_sentences, supported_sentences\n",
    "\n",
    "# Detect hallucinations in the generated summary\n",
    "hallucination_rate, unsupported_sentences, supported_sentences = detect_hallucinations_with_gpt(\n",
    "    summary, retriever\n",
    ")\n",
    "\n",
    "print(f\"\\nHallucination Rate: {hallucination_rate:.2f}%\")\n",
    "\n",
    "if unsupported_sentences:\n",
    "    print(\"\\nUnsupported Sentences:\")\n",
    "    for idx, (sent, explanation) in enumerate(unsupported_sentences, 1):\n",
    "        print(f\"{idx}. {sent}\\n   Explanation: {explanation}\")\n",
    "else:\n",
    "    print(\"\\nAll sentences are supported by the source documents.\")\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
