#  Boutique LLM Project with CEDS Documents

##  Repository Structure

Boutique-LLM-Basic/
├── LLM_Experiments/ # Experiments with different LLM configurations
│ ├── LLM_Basic_Mistral.ipynb / .py
│ ├── LLM_Bert_Mistral.ipynb / .py
│ ├── LLM_Bert_Agent_Mistral.ipynb / .py
│ └── LLM_Spacy_Mistral.ipynb / .py
├── Chunking_Experiments/ # Experiments on chunking strategies
│ ├── Chunking_note.ipynb / .py
│ ├── LLM_Customized_SMChunking.ipynb / .py
│ ├── LLM_NoCustomizing_Chunking.ipynb / .py
│ └── LLM_NoCustomizing_SMChunking.ipynb / .py
└── result_report.pdf
---

##  Project Purpose

This project aims to build a lightweight, domain-specific **boutique LLM pipeline** using the Mistral API and modern NLP components. It focuses on exploring different configurations of retrieval-augmented generation (RAG) pipelines and the impact of chunking strategies on performance and factual accuracy.

The repository consists of two main parts:

- **LLM Configuration Experiments**: Testing various combinations of LLMs and toolchains.
- **Chunking Strategy Experiments**: Exploring how different chunking techniques influence retrieval and summarization.

---

##  1. LLM Configuration Experiments

###  File Overview

- **LLM_Basic_Mistral**: A baseline RAG setup using LangChain and Mistral.
- **LLM_Bert_Mistral**: Adds named entity recognition (NER) using BERT to build a lightweight knowledge graph.
- **LLM_Bert_Agent_Mistral**: Enhances the pipeline with LangChain’s ReAct agent for interactive reasoning with knowledge graphs.
- **LLM_Spacy_Mistral**: Replaces BERT with spaCy for NER and relation extraction.

###  Common Workflow

1. Load and preprocess PDF documents using `PyPDFLoader` and `RecursiveCharacterTextSplitter`.
2. Generate embeddings with `HuggingFaceEmbeddings`.
3. Store document vectors in a `Chroma` vector database.
4. Use `RetrievalQA` powered by Mistral to answer natural language queries.
5. Optionally extract knowledge from text using BERT or spaCy and integrate it into responses.

###  Techniques Explored

- Named Entity Recognition (NER) with `dslim/bert-base-NER` and `spaCy`
- Heuristic-based relation extraction
- ReAct-style agentic reasoning using `AgentExecutor` and custom tools
- Simple knowledge graph construction and graph-aware QA

---

##  2. Chunking Strategy Experiments

This section investigates how different text chunking strategies impact the performance of LLM-based document understanding, especially in retrieval and summarization tasks.

###  File Overview

- **Chunking_note.ipynb**: Demonstrates token-level chunking using the Chonkie library with a GPT-2 tokenizer.
- **LLM_Customized_SMChunking.ipynb**: Applies a custom small/medium chunking method with overlap, tailored for coherent chunk generation.
- **LLM_NoCustomizing_Chunking.ipynb**: Uses GPT for semantic chunk refinement and applies hallucination detection.
- **LLM_NoCustomizing_SMChunking.ipynb**: Uses default chunking and cosine similarity to detect unsupported summaries.

---

###  Chunking Summary

####  Chunking_note.ipynb
- Demonstrates token-based chunking with `Chonkie` and `GPT2TokenizerFast`.
- Highlights differences between token- and character-based splitting.
- Tests various chunk sizes (e.g., 10 tokens) and overlap settings (e.g., 3 tokens).
- Explains tokenization artifacts like `Ġ` prefixes in GPT-2.

####  LLM_NoCustomizing_Chunking.ipynb
- Uses sentence-level chunking via NLTK.
- Refines each chunk using OpenAI's GPT-3.5 (`ChatCompletion`).
- Performs hallucination detection by comparing summaries with retrieved context.
- Reports hallucination rate and lists unsupported summary sentences.

####  LLM_NoCustomizing_SMChunking.ipynb
- Performs basic sentence-level chunking and GPT post-processing.
- Embeds summary sentences using `sentence-transformers/all-mpnet-base-v2`.
- Computes cosine similarity with context to detect hallucinations.
- Flags sentences below a similarity threshold (e.g., 0.7).

####  LLM_Customized_SMChunking.ipynb
- Defines a custom `AgenticChunker` class using Mistral and GPT2 tokenization.
- Groups sentences into semantically coherent chunks.
- Each chunk includes a title and summary generated by Mistral.
- Reuses existing chunks based on similarity or creates new ones.
- Final chunks are stored in Chroma and used for retrieval-based summarization.
- Hallucination detection is performed using top-k similarity comparisons.

---

##  Goal of Chunking Experiments

- Assess how chunk size, overlap, and structure affect retrieval precision and summary quality.
- Compare **default vs customized chunking** techniques.
- Analyze the impact of chunking on hallucination rates.
- Validate summaries using both LLM-based and embedding-based consistency checks.

---

##  Tech Stack

- **LLM**: [Mistral API](https://mistral.ai/)
- **LangChain**: Chains, agents, prompts, tools
- **Vector Store**: Chroma
- **NER Tools**: BERT (HuggingFace) and spaCy
- **Document Loader**: `PyPDFLoader`
- **Embeddings**: `HuggingFaceEmbeddings`
- **Chunking Libraries**: `RecursiveCharacterTextSplitter`, Chonkie, GPT-assisted chunking

---

##  Output Format

Each experiment includes:

- A Jupyter notebook (`.ipynb`)
- An executable Python script (`.py`)
- A summary or result report (`.pdf`)

---

##  How to Run

1. Install project requirements via `requirements.txt`.
2. Set your Mistral API key in a `.env` file:
MISTRAL_API_KEY=your_api_key_here
3. Run any `.py` script or open the relevant `.ipynb` file in Jupyter.

---

## ✍ Author’s Note

This repository serves as a technical portfolio and playground for building modular, small-scale LLM systems. It emphasizes practical experimentation, clarity, and adaptability for domain-specific applications.

> Contributions, suggestions, and questions are welcome!